<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="Interaction With Gaze, Gesture, and Speech in a Flexibly Configurable Augmented Reality System">
<meta name="author" content="Zhimin Wang">
<title>Interaction With Gaze, Gesture, and Speech in a Flexibly Configurable Augmented Reality System</title>
<!-- Bootstrap core CSS -->
<link href="./css/bootstrap.min.css" rel="stylesheet">
<!-- Custom styles for this template -->
<link href="./css/offcanvas.css" rel="stylesheet">
</head>

	
<body>
<div class="container">
<div class="jumbotron">
<h2>Interaction With Gaze, Gesture, and Speech in a Flexibly Configurable Augmented Reality System</h2>
<p class="abstract">THMS-2021</p>
<p iclass="authors"><b>Zhimin Wang</b>, Haofei Wang, Huangyue Yu, and Feng Lu </p>
<p>
   <a class="btn btn-primary" href="./pdf/wang21_THMS.pdf">PDF</a> 
</div>    


<!--teaser image-->	
<img src="./image/teaser.png" style="width:100%; margin-right:-20px; margin-top:-10px;">


<hr>
<div>
<h3>Abstract</h3>
<p>
  Multimodal interaction has become a recent research focus since it offers better user experience in augmented reality (AR) systems. However, most existing works only combine two modalities at a time, e.g., gesture and speech. Multimodal interactive system integrating gaze cue has rarely been investigated. In this article, we propose a multimodal interactive system that integrates gaze, gesture, and speech in a flexibly configurable AR system. Our lightweight head-mounted device supports accurate gaze tracking, hand gesture recognition, and speech recognition simultaneously. The system can be easily configured into various modality combinations, which enables us to investigate the effects of different interaction techniques.We evaluate the efficiency of these modalities using two tasks: the lamp brightness adjustment task and the cube manipulation task.We also collect subjective feedback when using such systems. The experimental results demonstrate that the Gaze+Gesture+Speech modality is superior in terms of efficiency, and the Gesture+Speech modality is more preferred by users. Our system opens the pathway toward a multimodal interactive AR system that enables flexible configuration.
</p>
</div>
	
	

<div class="section">
<h3>Paper Video</h3>
<object type='application/x-shockwave-flash' style='width:1024px; height:608px;' data='https://www.youtube.com/v/TDFcD7CDO70&t=1s'>
<param name='movie' value='https://www.youtube.com/v/TDFcD7CDO70&t=1s' />
</object>
</div>	

		
<div class="section">
<h3>Related Work</h3>
<hr>
<p>Our related work:</p>
<p><a href="https://zhimin-wang.github.io/GVC_See_Through_Vision.html.html">Gaze-Vergence-Controlled See-Through Vision in Augmented Reality</a></p>
<p><a href="https://zhimin-wang.github.io/publication/ismar_2021/pages/21_ISMAR.html">Edge-Guided Near-Eye Image Analysis for Head Mounted Displays</a></p>
</div>

	
<h3>Bibtex</h3>
<hr>
<div class="bibtexsection">
  @ARTICLE{wang_21THMS,
    author={Wang, Zhimin and Wang, Haofei and Yu, Huangyue and Lu, Feng},
    journal={IEEE Transactions on Human-Machine Systems}, 
    title={Interaction With Gaze, Gesture, and Speech in a Flexibly Configurable Augmented Reality System}, 
    year={2021},
    volume={51},
    number={5},
    pages={524-534},
    doi={10.1109/THMS.2021.3097973}}
</div>
		
    
<!-- <hr>
<footer>
<p>Send feedback and questions to <a href="https://cranehzm.github.io/">Zhiming Hu</a>.</p>
<p>Thanks to Vincent Sitzmann for his website template. Â© 2017</p>
</footer> -->


</div><!--/.container-->
</body></html>