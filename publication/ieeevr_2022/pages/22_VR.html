<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="Control with Vergence Eye Movement in Augmented Reality See-Through Vision">
<meta name="author" content="Zhimin Wang">
<title>Control with Vergence Eye Movement in Augmented Reality See-Through Vision</title>
<!-- Bootstrap core CSS -->
<link href="./css/bootstrap.min.css" rel="stylesheet">
<!-- Custom styles for this template -->
<link href="./css/offcanvas.css" rel="stylesheet">
</head>

	
<body>
<div class="container">
<div class="jumbotron">
<h2>Control with Vergence Eye Movement in Augmented Reality See-Through Vision</h2>
<p class="abstract">IEEE VRW, 2022</p>
<p iclass="authors"><b>Zhimin Wang</b>, Yuxin Zhao and Feng Lu </p>
<p>
   <a class="btn btn-primary" href="./pdf/wang22_VR.pdf">PDF</a> 
   <a class="btn btn-primary" href="./ppt/wang22_VR.pdf">PPT</a> 
</div>    


<!--teaser image-->	
<img src="./image/teaser.png" style="width:100%; margin-right:-20px; margin-top:-10px;">


<hr>
<div>
<h3>Abstract</h3>
<p>
  Augmented Reality (AR) see-through vision has become a recent research focus since it enables the user to see through a wall and see the occluded objects. Most existing works only used common modalities to control the display for see-through vision, e.g., button clicking and speech control. However, we use visual system to observe see-through vision. Using an addition interaction channel will distract the user and degrade the user experience. In this paper, we propose a novel interaction method using vergence eye movement for controlling see-through vision in AR. Specifically, we first customize eye cameras and design gaze depth estimation method for Microsoft HoloLens 2. With our algorithm, fixation depth can be computed from the vergence, and used to manage the see-through vision. We also propose two control techniques of gaze vergence. The experimental results show that the gaze depth estimation method is efficient. The difference cannot be found between these two modalities in terms of completion time and the number of successes.
</p>
</div>
	
	

<div class="section">
<h3>Presentation Video</h3>
<object type='application/x-shockwave-flash' style='width:1024px; height:608px;' data='https://www.youtube.com/v/Eycv9iuvfwg'>
<param name='movie' value='https://www.youtube.com/v/Eycv9iuvfwg' />
</object>
</div>	

		
<div class="section">
  <h3>Related Work</h3>
  <hr>
  <p>Our related work:</p>
  <p><a href="https://zhimin-wang.github.io/GVC_See_Through_Vision.html.html">Gaze-Vergence-Controlled See-Through Vision in Augmented Reality</a></p>
  <p><a href="https://zhimin-wang.github.io/publication/thms_2021/pages/21_THMS.html">Interaction With Gaze, Gesture, and Speech in a Flexibly Configurable Augmented Reality System</a></p>
  <p><a href="https://zhimin-wang.github.io/publication/ismar_2021/pages/21_ISMAR.html">Edge-Guided Near-Eye Image Analysis for Head Mounted Displays</a></p>
  </div>

	
<h3>Bibtex</h3>
<hr>
<div class="bibtexsection">
  @INPROCEEDINGS{wang_22VR,
    author={Wang, Zhimin and Zhao, Yuxin and Lu, Feng},
    booktitle={2022 IEEE Conference on Virtual Reality and 3D User Interfaces Abstracts and Workshops (VRW)}, 
    title={Control with Vergence Eye Movement in Augmented Reality See-Through Vision}, 
    year={2022},
    volume={},
    number={},
    pages={548-549},
    doi={10.1109/VRW55335.2022.00125}}
</div>
		
    
<!-- <hr>
<footer>
<p>Send feedback and questions to <a href="https://cranehzm.github.io/">Zhiming Hu</a>.</p>
<p>Thanks to Vincent Sitzmann for his website template. Â© 2017</p>
</footer> -->


</div><!--/.container-->
</body></html>