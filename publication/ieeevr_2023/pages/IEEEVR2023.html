<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="Exploring 3D Interaction with Gaze Guidance in Augmented Reality">
<meta name="author" content="Zhimin Wang">
<title>Exploring 3D Interaction with Gaze Guidance in Augmented Reality</title>
<!-- Bootstrap core CSS -->
<link href="./css/bootstrap.min.css" rel="stylesheet">
<!-- Custom styles for this template -->
<link href="./css/offcanvas.css" rel="stylesheet">
</head>

	
<body>
<div class="container">
<div class="jumbotron">
<h2>Exploring 3D Interaction with Gaze Guidance in Augmented Reality</h2>
<p class="abstract">IEEEVR-2023</p>
<p iclass="authors"> Yiwei Bao, Jiaxi Wang, <a href="https://zhimin-wang.github.io/">Zhimin Wang</a>,, and <a href="https://phi-ai.buaa.edu.cn/index.htm">Feng Lu</a> </p>
<p>
   <a class="btn btn-primary" href="./pdf/IEEEVR2023.pdf">PDF</a> 
</div>    


<!--teaser image-->	
<img src="./image/teaser.png" style="width:100%; margin-right:-20px; margin-top:-10px;">


<hr>
<div>
<h3>Abstract</h3>
<p>
  Recent research based on hand-eye coordination has shown that gaze could improve object selection and translation experience under certain scenarios in AR. However, several limitations still exist. Specifically, we investigate whether gaze could help object selection with heavy 3D occlusions and help 3D object translation in the depth dimension. In addition, we also investigate the possibility of reducing the gaze calibration burden before use. Therefore, we develop new methods with proper gaze guidance for 3D interaction in AR, and also an implicit online calibration method. We conduct two user studies to evaluate different interaction methods and the results show that our methods not only improve the effectiveness of occluded objects selection but also alleviate the arm fatigue problem significantly in the depth translation task. We also evaluate the proposed implicit online calibration method and find its accuracy comparable to standard 9 points explicit calibration, which makes a step towards practical use in the real world.
</p>
</div>
	
	

<div class="section">
<h3><a href="https://www.youtube.com/watch?v=GeRd7m04Xu0">30s Video</a></h3>
<object type='application/x-shockwave-flash' style='width:1024px; height:608px;' data='https://www.youtube.com/v/GeRd7m04Xu0'>
<param name='movie' value='https://www.youtube.com/v/GeRd7m04Xu0' />
</object>
</div>	

<div class="section">
  <h3><a href="https://www.youtube.com/v/hcSmCl2_haI">9min Video</a></h3>
  <object type='application/x-shockwave-flash' style='width:1024px; height:608px;' data='https://www.youtube.com/v/hcSmCl2_haI'>
  <param name='movie' value='https://www.youtube.com/v/hcSmCl2_haI' />
  </object>
  </div>	

		
<div class="section">
<h3>Related Work</h3>
<hr>
<p>Our related work:</p>
<p><a href="https://zhimin-wang.github.io/GVC_See_Through_Vision.html.html">Gaze-Vergence-Controlled See-Through Vision in Augmented Reality</a></p>
<p><a href="https://zhimin-wang.github.io/publication/ismar_2021/pages/21_ISMAR.html">Edge-Guided Near-Eye Image Analysis for Head Mounted Displays</a></p>
</div>

	
<h3>Bibtex</h3>
<hr>
<div class="bibtexsection">
  @INPROCEEDINGS{10108465,
    author={Bao, Yiwei and Wang, Jiaxi and Wang, Zhimin and Lu, Feng},
    booktitle={2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)}, 
    title={Exploring 3D Interaction with Gaze Guidance in Augmented Reality}, 
    year={2023},
    volume={},
    number={},
    pages={22-32},
    doi={10.1109/VR55154.2023.00018}}
</div>


</div><!--/.container-->
</body></html>