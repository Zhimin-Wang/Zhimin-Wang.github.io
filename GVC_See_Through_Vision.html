<!DOCTYPE html>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="description" content="Gaze-Vergence-Controlled See-Through Vision in Augmented Reality">
<meta name="author" content="Zhimin Wang">
<title>Gaze-Vergence-Controlled See-Through Vision in Augmented Reality</title>
<!-- Bootstrap core CSS -->
<link href="publication/ismar_journal_2022/pages/css/bootstrap.min.css" rel="stylesheet">
<!-- Custom styles for this template -->
<link href="publication/ismar_journal_2022/pages/css/offcanvas.css" rel="stylesheet">
</head>

	
<body>
<div class="container">
<div class="jumbotron">
<h2>Gaze-Vergence-Controlled See-Through Vision in Augmented Reality</h2>
<p class="abstract">IEEE TVCG, 2022.</p>
<p iclass="authors"><a href="https://zhimin-wang.github.io/">Zhimin Wang</a>, Yuxin Zhao and Feng Lu </p>
<p>
   <a class="btn btn-primary" href="./publication/ismar_journal_2022/pages/pdf/ISMAR2022journal.pdf">PDF</a>
   <a class="btn btn-primary" href="./publication/ismar_journal_2022/pages/ppt/ISMAR 2022.pdf">PDF</a>
   <a class="btn btn-primary" href="https://github.com/zhimin-wang/GVC-See-Through-Vision-in-AR">Code</a> 
</div>    


<!--teaser image-->	
<img src="publication/ismar_journal_2022/pages/image/teaser.png" style="width:100%; margin-right:-20px; margin-top:-10px;">


<hr>
<div>
<h3>Abstract</h3>
<p>
  Augmented Reality (AR) see-through vision is an interesting research topic since it enables users to see through a wall and see the occluded objects. Most existing research focuses on the visual effects of see-through vision, while the interaction method is less studied. However, we argue that using common interaction modalities, e.g., midair click and speech, may not be the optimal way to control see-through vision. This is because when we want to see through something, it is physically related to our gaze depth/vergence and thus should be naturally controlled by the eyes. Following this idea, this paper proposes a novel gaze-vergence-controlled (GVC) see-through vision technique in AR. Since gaze depth is needed, we build a gaze tracking module with two infrared cameras and the corresponding algorithm and assemble it into the Microsoft HoloLens 2 to achieve gaze depth estimation. We then propose two different GVC modes for see-through vision to fit different scenarios. Extensive experimental results demonstrate that our gaze depth estimation is efficient and accurate. By comparing with conventional interaction modalities, our GVC techniques are also shown to be superior in terms of efficiency and more preferred by users. Finally, we present four example applications of gaze-vergence-controlled see-through vision.
</p>
</div>
	
	

<div class="section">
<h3>Presentation Video: <a href="https://www.bilibili.com/video/BV1RB4y1H7M1?pop_share=1&vd_source=77e1c362aa8979736e123b0da448916f">Bilibili</a></h3>
<object type='application/x-shockwave-flash' style='width:1024px; height:608px;' data='https://www.youtube.com/v/OaUHD0JWe3s'>
<param name='movie' value='https://www.youtube.com/v/OaUHD0JWe3s' />
</object>
</div>	

		
<div class="section">
<h3>Related Work</h3>
<hr>
<p>Our related work:</p>
<p><a href="https://zhimin-wang.github.io/publication/thms_2021/pages/21_THMS.html">Interaction With Gaze, Gesture, and Speech in a Flexibly Configurable Augmented Reality System</a></p>
<p><a href="https://zhimin-wang.github.io/publication/ismar_2021/pages/21_ISMAR.html">Edge-Guided Near-Eye Image Analysis for Head Mounted Displays</a></p>
</div>

	
<h3>Bibtex</h3>
<hr>
<div class="bibtexsection">
  Comming!
</div>
		
    
<!-- <hr>
<footer>
<p>Send feedback and questions to <a href="https://cranehzm.github.io/">Zhiming Hu</a>.</p>
<p>Thanks to Vincent Sitzmann for his website template. Â© 2017</p>
</footer> -->


</div><!--/.container-->
</body></html>